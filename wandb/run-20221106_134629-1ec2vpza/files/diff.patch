diff --git a/__pycache__/parameters.cpython-37.pyc b/__pycache__/parameters.cpython-37.pyc
index c6b761b73..9c30ccfe0 100644
Binary files a/__pycache__/parameters.cpython-37.pyc and b/__pycache__/parameters.cpython-37.pyc differ
diff --git a/autoencoder/__pycache__/decoder.cpython-37.pyc b/autoencoder/__pycache__/decoder.cpython-37.pyc
index af8e3d2aa..7acc21aee 100644
Binary files a/autoencoder/__pycache__/decoder.cpython-37.pyc and b/autoencoder/__pycache__/decoder.cpython-37.pyc differ
diff --git a/autoencoder/__pycache__/encoder.cpython-37.pyc b/autoencoder/__pycache__/encoder.cpython-37.pyc
index 3f1af56e0..78e9f062e 100644
Binary files a/autoencoder/__pycache__/encoder.cpython-37.pyc and b/autoencoder/__pycache__/encoder.cpython-37.pyc differ
diff --git a/autoencoder/decoder.py b/autoencoder/decoder.py
index 54e47fe1b..e12ee4da4 100644
--- a/autoencoder/decoder.py
+++ b/autoencoder/decoder.py
@@ -2,7 +2,6 @@ import torch
 import torch.nn as nn
 
 
-torch.manual_seed(0)
 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
 
 class Decoder(nn.Module):
@@ -12,21 +11,21 @@ class Decoder(nn.Module):
 
         self.decoder_linear = nn.Sequential(
             nn.Linear(latent_dims, 1024),
-            nn.ReLU(),
+            nn.LeakyReLU(),
             nn.Linear(1024, 9 * 4 * 256),
-            nn.ReLU()
+            nn.LeakyReLU()
         )
 
         self.unflatten = nn.Unflatten(dim=1, unflattened_size=(256,4,9))
 
         self.decoder = nn.Sequential(
             nn.ConvTranspose2d(256, 128, 3, stride=2),
-            nn.ReLU(),
+            nn.LeakyReLU(),
             nn.ConvTranspose2d(128, 64, 4,  stride=2),
             nn.LeakyReLU(),
             nn.ConvTranspose2d(64, 32, 3, stride=2,
                                padding=1),
-            nn.ReLU(),
+            nn.LeakyReLU(),
             nn.ConvTranspose2d(32, 3, 4, stride=2),
             nn.Sigmoid())
         
diff --git a/autoencoder/encoder.py b/autoencoder/encoder.py
index 7f526e8ac..f98feff3b 100644
--- a/autoencoder/encoder.py
+++ b/autoencoder/encoder.py
@@ -3,7 +3,6 @@ import torch
 import torch.nn as nn
 
 
-torch.manual_seed(0)
 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
 
 class VariationalEncoder(nn.Module):
@@ -14,23 +13,25 @@ class VariationalEncoder(nn.Module):
 
         self.encoder_layer1 = nn.Sequential(
             nn.Conv2d(3, 32, 4, stride=2),  # 79, 39
-            nn.ReLU())
+            nn.LeakyReLU())
 
         self.encoder_layer2 = nn.Sequential(
             nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 40, 20
-            nn.ReLU())#,
+            nn.BatchNorm2d(64),
+            nn.LeakyReLU())
 
         self.encoder_layer3 = nn.Sequential(
             nn.Conv2d(64, 128, 4, stride=2),  # 19, 9
-            nn.ReLU())
+            nn.LeakyReLU())
 
         self.encoder_layer4 = nn.Sequential(
             nn.Conv2d(128, 256, 3, stride=2),  # 9, 4
-            nn.ReLU())
+            nn.BatchNorm2d(256),
+            nn.LeakyReLU())
 
         self.linear = nn.Sequential(
             nn.Linear(9*4*256, 1024),
-            nn.ReLU())
+            nn.LeakyReLU())
 
         self.mu = nn.Linear(1024, latent_dims)
         self.sigma = nn.Linear(1024, latent_dims)
diff --git a/autoencoder/model/var_encoder_model.pth b/autoencoder/model/var_encoder_model.pth
index f59a6e333..cf505a3a9 100644
Binary files a/autoencoder/model/var_encoder_model.pth and b/autoencoder/model/var_encoder_model.pth differ
diff --git a/autoencoder/variational_autoencoder.py b/autoencoder/variational_autoencoder.py
deleted file mode 100644
index f0cbe4a2e..000000000
--- a/autoencoder/variational_autoencoder.py
+++ /dev/null
@@ -1,116 +0,0 @@
-import os
-import sys
-import torch
-import torch.nn as nn
-import torch.optim as optim
-import torchvision.transforms as transforms
-from torchvision import datasets
-import torch.nn.functional as F
-from torch.utils.data import DataLoader, random_split
-from torch.utils.tensorboard import SummaryWriter
-from encoder import VariationalEncoder
-from decoder import Decoder
-
-
-# Hyper-parameters
-NUM_EPOCHS = 50
-BATCH_SIZE = 32
-LEARNING_RATE = 1e-4
-LATENT_SPACE = 400
-
-
-torch.manual_seed(0)
-device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
-
-
-class VariationalAutoencoder(nn.Module):
-    def __init__(self, latent_dims):
-        super(VariationalAutoencoder, self).__init__()
-        self.encoder = VariationalEncoder(latent_dims)
-        self.decoder = Decoder(latent_dims)
-
-    def forward(self, x):
-        x = x.to(device)
-        z = self.encoder(x)
-        return self.decoder(z)
-    
-    def save(self):
-        self.encoder.save()
-
-def train(model, trainloader, optim):
-    model.train()
-    train_loss = 0.0
-    for(x, _) in trainloader:
-        # Move tensor to the proper device
-        x = x.to(device)
-        x_hat = model(x)
-        loss = ((x - x_hat)**2).sum() + model.encoder.kl
-        optim.zero_grad()
-        loss.backward()
-        optim.step()
-        train_loss+=loss.item()
-    return train_loss / len(trainloader.dataset)
-
-
-def test(model, testloader):
-    # Set evaluation mode for encoder and decoder
-    model.eval()
-    val_loss = 0.0
-    with torch.no_grad(): # No need to track the gradients
-        for x, _ in testloader:
-            # Move tensor to the proper device
-            x = x.to(device)
-            # Encode data
-            encoded_data = model.encoder(x)
-            # Decode data
-            x_hat = model(x)
-            loss = ((x - x_hat)**2).sum() + model.encoder.kl
-            val_loss += loss.item()
-
-    return val_loss / len(testloader.dataset)
-
-
-def main():
-
-    data_dir = 'autoencoder/dataset/'
-
-    writer = SummaryWriter(f"runs/"+"auto-encoder-carla-0.9.08")
-    
-    # Applying Transformation
-    train_transforms = transforms.Compose([transforms.RandomRotation(30),transforms.RandomHorizontalFlip(),transforms.ToTensor()])
-    test_transforms = transforms.Compose([transforms.ToTensor()])
-
-    train_data = datasets.ImageFolder(data_dir+'train', transform=train_transforms)
-    test_data = datasets.ImageFolder(data_dir+'test', transform=test_transforms)
-    
-    m=len(train_data)
-    train_data, val_data = random_split(train_data, [int(m-m*0.2), int(m*0.2)])
-    
-
-    # Data Loading
-    trainloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
-    validloader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)
-    testloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE)
-    
-    model = VariationalAutoencoder(latent_dims=LATENT_SPACE).to(device)
-    optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
-    
-    print(f'Selected device :) :) :) {device}')
-
-    for epoch in range(NUM_EPOCHS):
-        train_loss = train(model,trainloader, optim)
-        writer.add_scalar("Training Loss/epoch", train_loss, epoch+1)
-        val_loss = test(model,validloader)
-        writer.add_scalar("Validation Loss/epoch", val_loss, epoch+1)
-        print('\nEPOCH {}/{} \t train loss {:.3f} \t val loss {:.3f}'.format(epoch + 1, NUM_EPOCHS,train_loss,val_loss))
-    
-    model.save()
-    #model.eval()
-
-if __name__ == "__main__":
-    try:
-        main()
-    except KeyboardInterrupt:
-        sys.exit()
-    finally:
-        print('\nTerminating...')
diff --git a/carla_driver.py b/carla_driver.py
index 7ee5996df..27435c8f0 100644
--- a/carla_driver.py
+++ b/carla_driver.py
@@ -15,13 +15,14 @@ from simulation.connection import ClientConnection
 from parameters import *
 from simulation.environment import CarlaEnvironment
 from networks.off_policy.ddqn.agent import DQNAgent
+from encoder_init import EncodeState
 
 
 def parse_args():
     
     parser = argparse.ArgumentParser()
     parser.add_argument('--exp-name', type=str, help='name of the experiment')
-    parser.add_argument('--env-name', type=str, default='carla-0.9.8', help='name of the simulation environment')
+    parser.add_argument('--env-name', type=str, default='carla', help='name of the simulation environment')
     parser.add_argument('--learning-rate', type=float, help='learning rate of the optimizer')
     parser.add_argument('--seed', type=int, default=0, help='seed of the experiment')
     parser.add_argument('--total-timesteps', type=int, default=EPISODES, help='total timesteps of the experiment')
@@ -35,6 +36,7 @@ def parse_args():
     return args
 
 
+
 def runner():
 
     #========================================================================
@@ -46,9 +48,9 @@ def runner():
     try:
 
         if exp_name == 'ddqn':
-            run_name = f"ddqn_{args.env_name}"
+            run_name = f"DDQN"
         elif exp_name == 'ppo':
-            run_name = f"ppo_{args.env_name}"
+            run_name = f"PPO"
 
     except Exception as e:
         print(e.message)
@@ -68,7 +70,7 @@ def runner():
             name=run_name,
             save_code=True,
         )
-        wandb.tensorboard.patch(root_logdir="runs/{run_name}", save=False, tensorboard_x=True, pytorch=True)
+        #wandb.tensorboard.patch(root_logdir="runs/{run_name}", save=False, tensorboard_x=True, pytorch=True)
     
     #Seeding to reproduce the results 
     random.seed(args.seed)
@@ -84,7 +86,7 @@ def runner():
     checkpoint_load = MODEL_LOAD
 
     if exp_name == 'ddqn':
-        n_actions = 11  # Car can only make 11 actions
+        n_actions = 7  # Car can only make 7 actions
         agent = DQNAgent(n_actions)
     elif exp_name == 'ppo':
         n_actions = 2  # Car can only make 2 actions
@@ -115,7 +117,6 @@ def runner():
                 epoch = data['epoch']
                 cumulative_score = data['cumulative_score']
 
-
     #========================================================================
     #                           CREATING THE SIMULATION
     #========================================================================
@@ -132,18 +133,21 @@ def runner():
         ConnectionRefusedError
 
     env = CarlaEnvironment(client, world)
+    encode = EncodeState(LATENT_DIM)
 
     #========================================================================
     #                           INITIALIZING THE MEMORY
     #========================================================================
     
-    if exp_name == 'ddqn':
+    if exp_name == 'ddqn' and checkpoint_load:
         while agent.replay_buffer.counter < agent.replay_buffer.buffer_size:
             observation = env.reset()
+            observation = encode.process(observation)
             done = False
             while not done:
                 action = random.randint(0,n_actions-1)
                 new_observation, reward, done, _ = env.step(action)
+                new_observation = encode.process(new_observation)
                 agent.save_transition(observation, action, reward, new_observation, int(done))
                 observation = new_observation
     
@@ -163,6 +167,7 @@ def runner():
             #Reset
             done = False
             observation = env.reset()
+            observation = encode.process(observation)
             score = 0
 
             #Episode start: timestamp
@@ -173,8 +178,8 @@ def runner():
                     action, prob, val = agent.get_action(observation)
                 else:
                     action = agent.get_action(observation)
-
                 new_observation, reward, done, _ = env.step(action)
+                new_observation = encode.process(new_observation)
                 score += reward
                 
                 if exp_name == 'ppo':
@@ -207,11 +212,11 @@ def runner():
 
                 if exp_name == 'ddqn':
                     data_obj = {'cumulative_score': cumulative_score, 'epsilon': agent.epsilon,'epoch': step}
-                    with open('checkpoint_DDQN.pickle', 'wb') as handle:
+                    with open('checkpoint_ddqn.pickle', 'wb') as handle:
                         pickle.dump(data_obj, handle)
                 elif exp_name == 'ppo':
                     data_obj = {'cumulative_score': cumulative_score,'epoch': step}
-                    with open('checkpoint_PPO.pickle', 'wb') as handle:
+                    with open('checkpoint_ppo.pickle', 'wb') as handle:
                         pickle.dump(data_obj, handle)
 
                 writer.add_scalar("Reward/info", np.mean(scores[-20:]), step)
diff --git a/networks/off_policy/ddqn/__pycache__/agent.cpython-37.pyc b/networks/off_policy/ddqn/__pycache__/agent.cpython-37.pyc
index a3de2a4f9..5a0d21718 100644
Binary files a/networks/off_policy/ddqn/__pycache__/agent.cpython-37.pyc and b/networks/off_policy/ddqn/__pycache__/agent.cpython-37.pyc differ
diff --git a/networks/off_policy/ddqn/__pycache__/dueling_dqn.cpython-37.pyc b/networks/off_policy/ddqn/__pycache__/dueling_dqn.cpython-37.pyc
index 81052c25c..2dd6b65c1 100644
Binary files a/networks/off_policy/ddqn/__pycache__/dueling_dqn.cpython-37.pyc and b/networks/off_policy/ddqn/__pycache__/dueling_dqn.cpython-37.pyc differ
diff --git a/networks/off_policy/ddqn/agent.py b/networks/off_policy/ddqn/agent.py
index bad29e9ca..1fc6ab8b8 100644
--- a/networks/off_policy/ddqn/agent.py
+++ b/networks/off_policy/ddqn/agent.py
@@ -16,7 +16,7 @@ class DQNAgent(object):
         self.mem_size = MEMORY_SIZE
         self.batch_size = BATCH_SIZE
         self.train_step = 0
-        self.replay_buffer = ReplayBuffer(MEMORY_SIZE,(203,), n_actions)
+        self.replay_buffer = ReplayBuffer(MEMORY_SIZE,100, n_actions)
         self.q_network_eval = DuelingDQnetwork(n_actions, MODEL_ONLINE)
         self.q_network_target = DuelingDQnetwork(n_actions, MODEL_TARGET)
 
@@ -25,7 +25,7 @@ class DQNAgent(object):
 
     def get_action(self, observation):
         if np.random.random() > self.epsilon:
-            observation = torch.tensor(observation, dtype=torch.float).to(self.q_network_eval.device)
+            #observation = torch.tensor(observation, dtype=torch.float).to(self.q_network_eval.device)
             _, advantage = self.q_network_eval.forward(observation)
             action = torch.argmax(advantage).item()
         else:
@@ -58,11 +58,11 @@ class DQNAgent(object):
         observation, action, reward, new_observation, done = self.replay_buffer.sample_buffer()
  
 
-        observation = torch.tensor(observation).to(self.q_network_eval.device)
-        action = torch.tensor(action).to(self.q_network_eval.device)
-        reward = torch.tensor(reward).to(self.q_network_eval.device)
-        new_observation = torch.tensor(new_observation).to(self.q_network_eval.device)
-        done = torch.tensor(done).to(self.q_network_eval.device)
+        observation = observation.to(self.q_network_eval.device)
+        action = action.to(self.q_network_eval.device)
+        reward = reward.to(self.q_network_eval.device)
+        new_observation = new_observation.to(self.q_network_eval.device)
+        done = done.to(self.q_network_eval.device)
 
 
         Vs, As = self.q_network_eval.forward(observation)
diff --git a/networks/off_policy/ddqn/dueling_dqn.py b/networks/off_policy/ddqn/dueling_dqn.py
index 679668ff1..24662139b 100644
--- a/networks/off_policy/ddqn/dueling_dqn.py
+++ b/networks/off_policy/ddqn/dueling_dqn.py
@@ -3,8 +3,8 @@ import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
-from parameters import DQN_LEARNING_RATE, DQN_CHECKPOINT_DIR, LATENT_DIM
-from autoencoder.variational_autoencoder import VariationalEncoder
+from parameters import DQN_LEARNING_RATE, DQN_CHECKPOINT_DIR
+
 
 class DuelingDQnetwork(nn.Module):
     def __init__(self, n_actions, model):
@@ -12,8 +12,8 @@ class DuelingDQnetwork(nn.Module):
         self.n_actions = n_actions
         self.checkpoint_file = os.path.join(DQN_CHECKPOINT_DIR, model)
 
-        self.Linear = nn.Sequential(
-            nn.Linear(200+3, 128),
+        self.Linear1 = nn.Sequential(
+            nn.Linear(95 + 5, 128),
             nn.ReLU(),
             nn.Linear(128, 64),
             nn.ReLU()
@@ -28,7 +28,7 @@ class DuelingDQnetwork(nn.Module):
         self.to(self.device)
 
     def forward(self, x):
-        fc = self.Linear(x)
+        fc = self.Linear1(x)
         V = self.V(fc)
         A = self.A(fc)
         return V, A
diff --git a/networks/off_policy/replay_buffer.py b/networks/off_policy/replay_buffer.py
index dfe0df13b..c6e16377f 100644
--- a/networks/off_policy/replay_buffer.py
+++ b/networks/off_policy/replay_buffer.py
@@ -1,4 +1,5 @@
 import numpy as np
+import torch
 from parameters import BATCH_SIZE
 
 
@@ -9,11 +10,11 @@ class ReplayBuffer(object):
         self.n_actions = n_actions
         self.buffer_size = max_size
         self.counter = 0
-        self.state_memory = np.zeros((self.buffer_size, observation), dtype=np.float32)
-        self.new_state_memory = np.zeros((self.buffer_size, observation), dtype=np.float32)
-        self.action_memory = np.zeros(self.buffer_size, dtype=np.int64)
-        self.reward_memory = np.zeros(self.buffer_size, dtype=np.float32)
-        self.terminal_memory = np.zeros(self.buffer_size, dtype=np.bool)
+        self.state_memory = torch.zeros((self.buffer_size, observation), dtype=torch.float32)
+        self.new_state_memory = torch.zeros((self.buffer_size, observation), dtype=torch.float32)
+        self.action_memory = torch.zeros(self.buffer_size, dtype=torch.int64)
+        self.reward_memory = torch.zeros(self.buffer_size, dtype=torch.float32)
+        self.terminal_memory = torch.zeros(self.buffer_size, dtype=torch.bool)
 
     def save_transition(self, state, action, reward, new_state, done):
 
diff --git a/networks/on_policy/ppo/__pycache__/agent.cpython-37.pyc b/networks/on_policy/ppo/__pycache__/agent.cpython-37.pyc
index e0862a249..356e17fab 100644
Binary files a/networks/on_policy/ppo/__pycache__/agent.cpython-37.pyc and b/networks/on_policy/ppo/__pycache__/agent.cpython-37.pyc differ
diff --git a/networks/on_policy/ppo/__pycache__/ppo.cpython-37.pyc b/networks/on_policy/ppo/__pycache__/ppo.cpython-37.pyc
index 203b18044..efb264eea 100644
Binary files a/networks/on_policy/ppo/__pycache__/ppo.cpython-37.pyc and b/networks/on_policy/ppo/__pycache__/ppo.cpython-37.pyc differ
diff --git a/parameters.py b/parameters.py
index 2e8f94323..4b8d153f3 100644
--- a/parameters.py
+++ b/parameters.py
@@ -10,13 +10,13 @@ IM_HEIGHT = 128
 GAMMA = 0.99
 MEMORY_SIZE = 10000
 EPISODES = 10000
-LATENT_DIM = 200
+LATENT_DIM = 95
 
 #Dueling DQN (hyper)parameters
 DQN_LEARNING_RATE = 0.0001
 EPSILON = 1.00
-EPSILON_END = 0.02
-EPSILON_DECREMENT = 0.0000002
+EPSILON_END = 0.05
+EPSILON_DECREMENT = 0.000001
 
 REPLACE_NETWORK = 10
 DQN_CHECKPOINT_DIR = 'models/ddqn'
diff --git a/simulation/connection.py b/simulation/connection.py
index 0358b6550..b650205c3 100644
--- a/simulation/connection.py
+++ b/simulation/connection.py
@@ -8,7 +8,7 @@ try:
         sys.version_info.minor,
         'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
 except IndexError:
-    pass
+    logging.error('Couldn\'t import Carla egg properly')
 
 import carla
 import logging
@@ -38,7 +38,7 @@ class ClientConnection:
     def __init__(self):
         self.client = None
 
-    def _setup(self):
+    def setup(self):
         try:
 
             # Connecting to the  Server
@@ -50,10 +50,10 @@ class ClientConnection:
         except Exception as e:
             logging.error(
                 'Failed to make a connection with the server: {}'.format(e))
-            self._error_log()
+            self.error_log()
 
     # An error log method: prints out the details if the client failed to make a connection
-    def _error_log(self):
+    def error_log(self):
 
         logging.debug("\nClient version: {}".format(
             self.client.get_client_version()))
diff --git a/simulation/environment.py b/simulation/environment.py
index 0b88efe99..09aa7de38 100644
--- a/simulation/environment.py
+++ b/simulation/environment.py
@@ -51,7 +51,6 @@ class CarlaEnvironment():
                 self.actor_list.clear()
             self.remove_sensors()
 
-
             # Spawn points of the entire map!
             #spawn_points = self.map.get_spawn_points()
 
@@ -60,7 +59,11 @@ class CarlaEnvironment():
 
             #vehicle = self.actor_vehicle(vehicle_bp, spawn_points)
             #self.set_vehicle(vehicle_bp, spawn_points)
-            self.start_waypoint = self.world.map.get_waypoint(self.map.get_spawn_points()[1].location)
+            transform = self.map.get_spawn_points()[1]
+            transform.location += carla.Location(z=0.2)
+            self.vehicle = self.world.try_spawn_actor(vehicle_bp, transform)
+            self.actor_list.append(self.vehicle)
+
 
             # Camera Sensor
             self.camera_obj = CameraSensor(self.vehicle)
@@ -74,7 +77,6 @@ class CarlaEnvironment():
                 self.env_camera_obj = CameraSensorEnv(self.vehicle)
                 self.sensor_list.append(self.env_camera_obj.sensor)
 
-
             # Collision sensor
             self.collision_obj = CollisionSensor(self.vehicle)
             self.collision_history = self.collision_obj.collision_data
@@ -100,10 +102,12 @@ class CarlaEnvironment():
             self.throttle = float(0.0000000)
             self.previous_steer = float(0.0000000)
             self.velocity = float(0.0000000)
+            self.distance_from_center = float(0.000000)
+            self.angle = float(0.000000)
+
 
-            
             # Waypoint nearby angle and distance from it
-            self.waypoint = self.map.get_waypoint(self.start_waypoint.transform.location, project_to_road=True, lane_type=(carla.LaneType.Driving))
+            self.waypoint = self.map.get_waypoint(self.vehicle.get_location(), project_to_road=True, lane_type=(carla.LaneType.Driving))
             current_waypoint = self.waypoint
             self.route_waypoints.append(current_waypoint)
             for _ in range(self.total_distance):
@@ -114,30 +118,30 @@ class CarlaEnvironment():
             if not from_start:
             # Teleport vehicle to last checkpoint
                 waypoint = self.route_waypoints[self.checkpoint_waypoint_index % len(self.route_waypoints)]
-                waypoint.location += carla.Location(z=1.0)
-                self.vehicle = self.world.try_spawn_actor(vehicle_bp, waypoint.transform)
-                self.actor_list.append(self.vehicle)
+                transform = waypoint.transform
+                transform.location += carla.Location(z=1.0)
+                self.vehicle.set_transform(transform)
                 self.current_waypoint_index = self.checkpoint_waypoint_index
+
             else:
-                waypoint = self.route_waypoints[0]
+                #waypoint = self.route_waypoints[0]
                 self.current_waypoint_index = 0
                 transform = self.waypoint.transform
                 transform.location += carla.Location(z=1.0)
-                self.vehicle = self.world.try_spawn_actor(vehicle_bp, transform)
-                self.actor_list.append(self.vehicle)
-                #self.vehicle.set_transform(transform)
+                #self.vehicle = self.world.try_spawn_actor(vehicle_bp, transform)
+                #self.actor_list.append(self.vehicle)
+                self.vehicle.set_transform(transform)
                 self.vehicle.set_simulate_physics(False)
                 self.vehicle.set_simulate_physics(True)
             
             self.start_waypoint_index = self.current_waypoint_index
             
             self.collision_history.clear()
-            self.navigation_obs = np.array([self.throttle, self.velocity, self.previous_steer])
+            self.navigation_obs = np.array([self.throttle, self.velocity, self.previous_steer, self.distance_from_center, self.angle])
 
             logging.info("Environment has been resetted.")
             self.episode_start_time = time.time()
-            observation = np.concatenate(self.image_obs.flatten(), self.navigation_obs)
-            return observation
+            return [self.image_obs, self.navigation_obs]
 
         except:
             self.client.apply_batch([carla.command.DestroyActor(x) for x in self.sensor_list])
@@ -159,22 +163,21 @@ class CarlaEnvironment():
 
             # Velocity of the vehicle
             velocity = self.vehicle.get_velocity()
-            self.velocity = math.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2) * 3.6
-
+            self.velocity = np.sqrt(velocity.x**2 + velocity.y**2 + velocity.z**2) * 3.6
 
             # Action fron action space for contolling the vehicle with a discrete action
             if self.continous_action_space:
                 self.vehicle.apply_control(carla.VehicleControl(steer=float(action_idx[0]), throttle=float(action_idx[1])))
                 self.prev_steer = float(action_idx[0])
             else:
+                
                 action = self.action_space[action_idx]
                 if self.velocity < 20.0:
-                    self.vehicle.apply_control(carla.VehicleControl(throttle=0.2, steer=action))
+                    self.vehicle.apply_control(carla.VehicleControl(steer=action, throttle=float(0.5)))
                 else:
                     self.vehicle.apply_control(carla.VehicleControl(steer=action))
                 self.prev_steer = action
         
-
             # Traffic Light state
             if self.vehicle.is_at_traffic_light():
                 traffic_light = self.vehicle.get_traffic_light()
@@ -208,34 +211,32 @@ class CarlaEnvironment():
                     break
 
             self.current_waypoint_index = waypoint_index
-
             # Calculate deviation from center of the lane
             self.current_waypoint = self.route_waypoints[ self.current_waypoint_index    % len(self.route_waypoints)]
             self.next_waypoint = self.route_waypoints[(self.current_waypoint_index+1) % len(self.route_waypoints)]
-            distance_from_center = self.distance_to_line(self.vector(self.current_waypoint.transform.location),self.vector(self.next_waypoint.transform.location),self.vector(self.location))
+            self.distance_from_center = self.distance_to_line(self.vector(self.current_waypoint.transform.location),self.vector(self.next_waypoint.transform.location),self.vector(self.location))
             #self.center_lane_deviation += self.distance_from_center
 
             # Get angle difference between closest waypoint and vehicle forward vector
             fwd    = self.vector(self.vehicle.get_velocity())
             wp_fwd = self.vector(self.current_waypoint.transform.rotation.get_forward_vector())
-            angle  = self.angle_diff(fwd, wp_fwd)
+            self.angle  = self.angle_diff(fwd, wp_fwd)
             
 
             self.distance_traveled += (math.sqrt((self.previous_location.x - self.location.x)**2 + (self.previous_location.y - self.location.y)**2))
             #self.previous_location.distance(self.location)
             self.previous_location = self.location
 
-            #self.desired_distance = (self.current_waypoint_index - self.start_waypoint_index) / len(self.route_waypoints)
-            if self.distance_traveled >= self.total_distance:
+            self.desired_distance = (self.current_waypoint_index - self.start_waypoint_index) / len(self.route_waypoints)
+            if self.desired_distance >= 1:
                 done = True
                     
             # Update checkpoint for training
             if not self.from_start:
-                checkpoint_frequency = 50
+                checkpoint_frequency = 30
                 self.checkpoint_waypoint_index = (self.current_waypoint_index // checkpoint_frequency) * checkpoint_frequency
             
             # Rewards are given below!
-
             done = False
             reward = 0
 
@@ -243,7 +244,7 @@ class CarlaEnvironment():
                 done = True
                 reward = -10
                 logging.warning("Vehicle has collided.")
-            elif distance_from_center > self.max_distance_from_center:
+            elif self.distance_from_center > self.max_distance_from_center:
                 done = True
                 reward = -10
                 logging.warning("Vehicle has gone out of the lane.")
@@ -259,9 +260,9 @@ class CarlaEnvironment():
 
             if not done:
                 # Interpolated from 1 when centered to 0 when 3 m from center
-                centering_factor = max(1.0 - distance_from_center / self.max_distance_from_center, 0.0)
+                centering_factor = max(1.0 - self.distance_from_center / self.max_distance_from_center, 0.0)
                 # Interpolated from 1 when aligned with the road to 0 when +/- 20 degress of road
-                angle_factor = max(1.0 - abs(angle / np.deg2rad(20)), 0.0)
+                angle_factor = max(1.0 - abs(self.angle / np.deg2rad(20)), 0.0)
 
                 if self.velocity < self.min_speed:
                     reward = (self.velocity / self.min_speed) * centering_factor * angle_factor    
@@ -270,11 +271,11 @@ class CarlaEnvironment():
                 else:                                         
                     reward = 1.0 * centering_factor * angle_factor                 
 
-
-            while (self.front_camera is None):
+            while(len(self.camera_obj.front_camera) == 0):
                 time.sleep(0.001)
+            self.image_obs = self.camera_obj.front_camera.pop(-1)
 
-            self.navigation_obs = np.array([self.throttle, self.velocity, self.previous_steer])
+            self.navigation_obs = np.array([self.throttle, self.velocity, self.previous_steer, self.distance_from_center, self.angle])
             # Remove everything that has been spawned in the env
             if done:
                 for sensor in self.sensor_list:
@@ -282,8 +283,7 @@ class CarlaEnvironment():
                 self.remove_sensors()
                 for actor in self.actor_list:
                     actor.destroy()
-            observation = np.concatenate(self.image_obs.flatten(), self.navigation_obs)
-            return observation, reward, done, None
+            return [self.image_obs, self.navigation_obs], reward, done, None
 
         except:
             self.client.apply_batch([carla.command.DestroyActor(x) for x in self.sensor_list])
@@ -423,7 +423,14 @@ class CarlaEnvironment():
 
     def get_discrete_action_space(self):
         action_space = \
-            np.array([-1.0, -0.75, -0.50, -0.25, -0.10, 0.0, 0.10, 0.25, 0.50, 0.75, 1.0])
+            np.array([
+            -0.50,
+            -0.30,
+            -0.10,
+             0.0,
+             0.10,
+             0.30,
+             0.50])
         return action_space
 
     # Main vehicle blueprint method
diff --git a/simulation/sensors.py b/simulation/sensors.py
index 5de25b593..723495465 100644
--- a/simulation/sensors.py
+++ b/simulation/sensors.py
@@ -1,9 +1,7 @@
 import math
-from autoencoder.encoder import VariationalEncoder
 import numpy as np
 import weakref
 import logging
-from parameters import LATENT_DIM
 import pygame
 from simulation.connection import carla
 from simulation.settings import RGB_CAMERA, SSC_CAMERA
@@ -21,15 +19,6 @@ class CameraSensor():
         self.front_camera = list()
         world = self.parent.get_world()
         self.sensor = self._set_camera_sensor(world)
-
-        self.conv_encoder = VariationalEncoder(LATENT_DIM)
-        self.conv_encoder.load()
-        
-        
-        self.conv_encoder.eval()
-        for params in self.conv_encoder.parameters():
-            params.requires_grad = False
-
         weak_self = weakref.ref(self)
         self.sensor.listen(
             lambda image: CameraSensor._get_front_camera_data(weak_self, image))
@@ -37,8 +26,8 @@ class CameraSensor():
     # Main front camera is setup and provide the visual observations for our network.
     def _set_camera_sensor(self, world):
         front_camera_bp = world.get_blueprint_library().find(self.sensor_name)
-        front_camera_bp.set_attribute('image_size_x', f'128')
-        front_camera_bp.set_attribute('image_size_y', f'128')
+        front_camera_bp.set_attribute('image_size_x', f'160')
+        front_camera_bp.set_attribute('image_size_y', f'80')
         front_camera_bp.set_attribute('fov', f'125')
         front_camera = world.spawn_actor(front_camera_bp, carla.Transform(
             carla.Location(x=2.4, z=1.5), carla.Rotation(pitch= -10)), attach_to=self.parent)
@@ -53,12 +42,9 @@ class CameraSensor():
         placeholder = np.frombuffer(image.raw_data, dtype=np.dtype("uint8"))
         placeholder1 = placeholder.reshape((image.width, image.height, 4))
         target = placeholder1[:, :, :3]
-        self.conv_encoder(target)
         self.front_camera.append(target)#/255.0)
 
 
-
-
 # ---------------------------------------------------------------------|
 # ------------------------------- ENV CAMERA |
 # ---------------------------------------------------------------------|
diff --git a/simulation/settings.py b/simulation/settings.py
index 53e6ccab4..6dfd9f168 100644
--- a/simulation/settings.py
+++ b/simulation/settings.py
@@ -16,7 +16,7 @@ CAR_NAME = 'model3'
 EPISODE_LENGTH = 120
 NUMBER_OF_VEHICLES = 30
 NUMBER_OF_PEDESTRIAN = 100
-CONTINUOUS_ACTION = True
+CONTINUOUS_ACTION = False
 VISUAL_DISPLAY = True
 
 
